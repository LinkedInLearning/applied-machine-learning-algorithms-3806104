{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms\n",
    "\n",
    "Going to provide an overview of the most common machine learning algorithms.\n",
    "\n",
    "You should feel comfortable with:\n",
    "\n",
    "- Jupyter Notebooks\n",
    "- Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering\n",
    " \n",
    "K-means clustering is an unsupervised learning algorithm that classifies a data point based on the majority of its neighbors. \n",
    "\n",
    "The algorithm is as follows:\n",
    "\n",
    "- Choose the number of $k$ and a distance metric.\n",
    "- Find the $k$ nearest neighbors of the sample that we want to classify.\n",
    "- Assign the class label by majority vote.\n",
    "- Update the centroids of each class.\n",
    "- Repeat the steps above until convergence.\n",
    "\n",
    "Note that because this algorithm takes distance into account, it is important that the features (columns) are on the same scale. For the iris dataset, they are but for other datasets, you may need to scale the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import sklearn.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = sklearn.datasets.load_iris(as_frame=True)\n",
    "\n",
    "X = (dataset\n",
    "     ['data']\n",
    "     .loc[:, ['sepal length (cm)', 'sepal width (cm)']]\n",
    ")\n",
    "y = dataset['target']\n",
    "\n",
    "# demonstration of k-means clustering with iris dataset\n",
    "# keep list of all centroids\n",
    "centroids = []\n",
    "\n",
    "# loop over a few iterations and plot the results\n",
    "for i in range(10):\n",
    "    model = KMeans(n_clusters=3, init='random', n_init=1,\n",
    "                    max_iter=i+1, random_state=42)\n",
    "    model.fit(X)\n",
    "    label = model.predict(X)\n",
    "    # plot the input data color by cluster\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    X.plot.scatter(x='sepal length (cm)', y='sepal width (cm)', c=label, cmap='viridis', ax=ax)\n",
    "    # plot the centers\n",
    "    centers = model.cluster_centers_\n",
    "    ax.scatter(centers[:, 0], centers[:, 1], marker='*', s=250, color='r', alpha=.5)\n",
    "    ax.set_title('Iteration: ' + str(i))\n",
    "    # plot previous centroids with reduced alpha value\n",
    "    if i > 0:\n",
    "        for centroid in centroids:\n",
    "            ax.scatter(centroid[:, 0], centroid[:, 1], marker='*', s=250, color='r', alpha=.1)\n",
    "    # save the current centroids\n",
    "    centroids.append(model.cluster_centers_)\n",
    "    \n",
    "  \n",
    "  # plot the original data color by target\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "X.plot.scatter(x='sepal length (cm)', y='sepal width (cm)', c=y, cmap='viridis', ax=ax)\n",
    "ax.set_title('Original Data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Evaluation for K-means\n",
    "\n",
    "We specify the K value for K-means clustering. However, we do not know the best K value for a given dataset. \n",
    "\n",
    "There are a few methods to evaluate the K value for K-means clustering:\n",
    "\n",
    "- Elbow method - Track the \"inertia\" of the model as the K value increases. The inertia is the sum of squared distances of samples to their closest cluster center. \n",
    "- Silhouette coefficient - The silhouette coefficient is a measure of how similar an object is to its own cluster compared to other clusters. The silhouette coefficient ranges from -1 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run elbow method to find optimal number of clusters\n",
    "\n",
    "inertias = []\n",
    "max_clusters = 20\n",
    "for i in range(max_clusters):\n",
    "    km = KMeans(n_clusters=i+1, n_init='auto',\n",
    "                max_iter=300, random_state=42)\n",
    "    km.fit(X)\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "# plot the results\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(range(1, max_clusters+1), inertias, marker='o')\n",
    "ax.set_xlabel('Number of clusters')\n",
    "ax.set_ylabel('Inertia')\n",
    "ax.set_title('Elbow Method')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SilhouetteVisualizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list matplotlib fonts available\n",
    "import matplotlib.font_manager\n",
    "matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if on linux, set matplotlib font to DejaVuSans\n",
    "# get rid of warnings in next cell\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = 'DejaVu Sans'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run silhouette method to find optimal number of clusters\n",
    "from yellowbrick.cluster import silhouette_visualizer \n",
    "\n",
    "max_clusters = 6\n",
    "for i in range(max_clusters):\n",
    "    km = KMeans(n_clusters=i+2, n_init='auto',\n",
    "                max_iter=300, random_state=42)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    # setting show=False so we can set xlim to same value for all plots\n",
    "    viz = silhouette_visualizer(km, X, colors='yellowbrick', ax=ax, show=False)\n",
    "    ax.set_xlim([-0.1, .8])    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Clustering Results\n",
    "\n",
    "We can use the following methods to understand the clustering results:\n",
    "\n",
    "- Create a surrogate model to predict the cluster label for a given sample.\n",
    "- Summarize the cluster by the mean of each feature.\n",
    "- Visualize the clustering results in 2D or 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to cluster electricity data from Australia\n",
    "# https://www.openml.org/search?type=data&sort=runs&id=151&status=active\n",
    "from datasets import load_dataset\n",
    "electricity = load_dataset('inria-soda/tabular-benchmark', data_files='clf_num/electricity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(electricity['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec = electricity['train'].to_pandas()\n",
    "elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = elec.drop(columns=['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inertias for different numbers of clusters\n",
    "inertias = []\n",
    "max_clusters = 20\n",
    "for i in range(max_clusters):\n",
    "    km = KMeans(n_clusters=i+1, n_init='auto',\n",
    "                max_iter=300, random_state=42)\n",
    "    km.fit(X)\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "# plot the results\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(range(1, max_clusters+1), inertias, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 2min 20s w/o sampling\n",
    "# get silhouette scores for different numbers of clusters\n",
    "from yellowbrick.cluster import silhouette_visualizer\n",
    "\n",
    "max_clusters = 6\n",
    "for i in range(max_clusters):\n",
    "    km = KMeans(n_clusters=i+2, n_init='auto',\n",
    "                max_iter=300, random_state=42)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    # setting show=False so we can set xlim to same value for all plots\n",
    "    viz = silhouette_visualizer(km, X.sample(1_000, random_state=42), colors='yellowbrick', ax=ax, show=False)\n",
    "    ax.set_xlim([-0.1, .8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to choose 5 clusters (if 5, 6, or 7 are all close, choose the simpler model)\n",
    "# summarize the results by group\n",
    "\n",
    "km = KMeans(n_clusters=5, n_init='auto',\n",
    "            max_iter=300, random_state=42)\n",
    "km.fit(X)\n",
    "label = km.predict(X)\n",
    "(elec\n",
    " .assign(cluster=label)\n",
    " .groupby('cluster')\n",
    " .agg('mean', numeric_only=True)\n",
    " .T\n",
    " .style\n",
    " .background_gradient(cmap='RdBu', axis='columns')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(km))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize by surrogate model decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dt.fit(X, label)\n",
    "# plot the tree\n",
    "from sklearn.tree import plot_tree\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# make string for class names\n",
    "class_names = [str(i) for i in range(0, 5)]\n",
    "_ = plot_tree(dt, ax=ax, feature_names=X.columns, class_names=class_names, filled=True, fontsize=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Clustering Algorithms\n",
    "\n",
    "- Hierarchical clustering - This clusters by distance. It first clusters the two closest points, then clusters the next closest point to the first cluster, and so on. It is not as efficient as k-means, but it does not require you to specify the number of clusters. It is also more robust to outliers than k-means.\n",
    "- DBSCAN -  This clusters by density. It finds the densest region of points and clusters them together. It is also more robust to outliers than k-means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: K-means Clustering\n",
    "\n",
    "With the Titanic dataset:\n",
    "- drop the missing values\n",
    "- drop the categorical features\n",
    "- scale the features\n",
    "- run K-means clustering\n",
    "- use the elbow method to find the best K value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: K-means Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to PCA\n",
    "\n",
    "Principal component analysis (PCA) is a dimensionality reduction technique. It finds the linear combinations of the features that explain the most variance in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import sklearn.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = sklearn.datasets.load_iris(as_frame=True)\n",
    "\n",
    "X = (dataset\n",
    "     ['data']\n",
    "     .loc[:, ['sepal length (cm)', 'sepal width (cm)']]\n",
    ")\n",
    "y = dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA on the first two components of the Iris dataset\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "# plot the results\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_title('PCA on Iris Data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA on all of the Iris dataset\n",
    "pca = PCA()\n",
    "\n",
    "X_all = (dataset\n",
    "     ['data']\n",
    "     #.loc[:, ['sepal length (cm)', 'sepal width (cm)']]\n",
    ")\n",
    "pca.fit(X_all)\n",
    "X_pca = pca.transform(X_all)\n",
    "\n",
    "# Plot to first 2 components\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_title('PCA on Iris Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of PCA\n",
    "\n",
    "When we run PCA, we get back the following:\n",
    "\n",
    "- Principal components (PCs): The PCs are the linear combinations of the features. \n",
    "- Explained variance ratio: The explained variance ratio tells us how much variance is explained by each PC.\n",
    "- Feature weights: The feature weights tell us how much each feature contributes to each PC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell sklearn to output pandas dataframes\n",
    "sklearn.set_config(transform_output='pandas')\n",
    "\n",
    "pca.transform(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't like the column names, so I'll rename them\n",
    "# change pca0 to PC1, pca1 to PC2, etc\n",
    "\n",
    "def rename_pc0_to_PC1(col):\n",
    "    num = int(col[3:]) + 1\n",
    "    return 'PC' + str(num)\n",
    "\n",
    "pca.transform(X_all).rename(columns=rename_pc0_to_PC1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree plot of explained variance\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(range(1, len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_, marker='o')\n",
    "ax.set_xlabel('Principal Component')\n",
    "ax.set_ylabel('Explained Variance Ratio')\n",
    "ax.set_title('Scree Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The explained variance ratio is the percentage of variance explained by each of the selected components.\n",
    "# The first principal component explains 92.5% of the variance in the data, and \n",
    "# the second principal component explains 5.3% of the variance in the data.\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights\n",
    "\n",
    "For every PC, we get a set of weights for each feature. The weights tell us how much each feature contributes to the PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert components to a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "components = pd.DataFrame(pca.components_, columns=X_all.columns,\n",
    "                          index=['PC1', 'PC2', 'PC3', 'PC4'])\n",
    "                          \n",
    "components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centered data - for next cell's calculation\n",
    "X_all - X_all.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating PC1 by hand for first row - linear combination of \n",
    "# centered variables and the first component \n",
    "-.743333 * .3613 + 0.4426 * -0.0845 + -2.358 * 0.8566 + -0.9993 * 0.3582"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.transform(X_all).rename(columns=rename_pc0_to_PC1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually calculating PCA with numpy\n",
    "import numpy as np\n",
    "nums = X_all - X_all.mean()\n",
    "vals, vecs = np.linalg.eig(nums.cov())\n",
    "idxs = pd.Series(vals).argsort()\n",
    "\n",
    "explained_variance = pd.Series(sorted(vals, reverse=True))\n",
    "\n",
    "def set_columns(df_):\n",
    "    df_.columns = [f'PC{i+1}' for i in range(len(df_.columns))]\n",
    "    return df_\n",
    "\n",
    "comps = (pd.DataFrame(vecs, index=nums.columns)\n",
    " .iloc[:, idxs[::-1]]\n",
    " .pipe(set_columns)\n",
    ")\n",
    "\n",
    "pcas = (nums.dot(comps))\n",
    "pcas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use plotly to plot the first three components\n",
    "import plotly.express as px\n",
    "fig = px.scatter_3d(pcas, x='PC1', y='PC2', z='PC3', color=y)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Dimensionality Reduction Techniques\n",
    "\n",
    "- t-SNE - t-distributed stochastic neighbor embedding. Tries to preserve the local structure of the data.\n",
    "- UMAP - Uniform Manifold Approximation and Projection. Tries to preserve the both global and structure of the data.\n",
    "- Autoencoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run t-SNE on the Iris dataset\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results with plotly\n",
    "import plotly.express as px\n",
    "fig = px.scatter_3d(X_tsne, x='tsne0', y='tsne1', z='tsne2', color=y)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run UMAP\n",
    "import umap\n",
    "reducer = umap.UMAP(random_state=42, n_components=3)\n",
    "X_umap = pd.DataFrame(reducer.fit_transform(X_all), columns=['umap0', 'umap1', 'umap2'])\n",
    "X_umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results with plotly\n",
    "import plotly.express as px\n",
    "fig = px.scatter_3d(X_umap, x='umap0', y='umap1', z='umap2', color=y)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: PCA\n",
    "\n",
    "Run PCA on the numeric columns of the Titanic data. Plot the result of the first three dimensons using plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Algorithm\n",
    "\n",
    "Linear regression calculates an intercept and slope (weights) for a line that minimizes the sum of squared errors between the line and the data points.\n",
    "\n",
    "The formula for linear regression is as follows:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n$$\n",
    "\n",
    "where $y$ is the target variable, $\\beta_0$ is the intercept, $\\beta_1$ to $\\beta_n$ are the weights, and $x_1$ to $x_n$ are the features.\n",
    "\n",
    "The algorithm is as follows:\n",
    "\n",
    "- Initialize the weights.\n",
    "- Calculate the predicted values.\n",
    "- Calculate the error.\n",
    "- Update the weights.\n",
    "- Repeat the steps above until convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load anscombe's quartet\n",
    "x = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\n",
    "y1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]\n",
    "y2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74]\n",
    "y3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]\n",
    "x4 = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8]\n",
    "y4 = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n",
    "anscombe = (pd.DataFrame({'x': x, 'y1': y1, 'y2': y2, 'y3': y3, 'x4': x4, 'y4': y4})\n",
    "            )\n",
    "\n",
    "anscombe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot x y1\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "anscombe.plot.scatter(x='x', y='y1', ax=ax, color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the algorithm on x and y1\n",
    "\n",
    "Calculate the slope:\n",
    "\n",
    "$$\\beta_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}$$\n",
    "\n",
    "Calculate the intercept:\n",
    "\n",
    "$$\\beta_0 = \\bar{y} - \\beta_1 \\bar{x}$$\n",
    "\n",
    "Model Equation:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slope\n",
    "\n",
    "x1 = anscombe['x']\n",
    "y1 = anscombe['y1']\n",
    "slope = ((x1 - x1.mean())*(y1 - y1.mean())).sum() / ((x1 - x1.mean())**2).sum()\n",
    "slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intercept\n",
    "\n",
    "intercept = y1.mean() - slope * x1.mean()\n",
    "intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot x y1\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "anscombe.plot.scatter(x='x', y='y1', ax=ax, color='k')\n",
    "# plot the line\n",
    "x1 = np.linspace(4, 14, 100)\n",
    "y1 = slope * x1 + intercept\n",
    "ax.plot(x1, y1, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples with Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x1 = anscombe[['x']]\n",
    "y1 = anscombe['y1']\n",
    "y2 = anscombe['y2']\n",
    "y3 = anscombe['y3']\n",
    "\n",
    "lr1 = LinearRegression()\n",
    "lr1.fit(x1, y1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LinearRegression()\n",
    "lr2.fit(x1, y2)\n",
    "lr3 = LinearRegression()\n",
    "lr3.fit(x1, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 1, 2 and 3 in different colors\n",
    "fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "anscombe.plot.scatter(x='x', y='y1', ax=axs[0], color='k')\n",
    "axs[0].plot(x1, lr1.predict(x1), color='#aaa')\n",
    "axs[0].set_ylim(3, 13)\n",
    "anscombe.plot.scatter(x='x', y='y2', ax=axs[1], color='b')\n",
    "axs[1].plot(x1, lr2.predict(x1), color='#55a')\n",
    "axs[1].set_ylim(3, 13)\n",
    "anscombe.plot.scatter(x='x', y='y3', ax=axs[2], color='g')\n",
    "axs[2].plot(x1, lr3.predict(x1), color='#5a5')\n",
    "axs[2].set_ylim(3, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real world example with Aircraft Elevators\n",
    "\n",
    "From website: This data set is also obtained from the task of controlling a F16 aircraft, although the target variable and attributes are different from the ailerons domain. In this case the goal variable is related to an action taken on the elevators of the aircraft.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.openml.org/search?type=data&sort=runs&id=216&satatus=active \n",
    "from datasets import load_dataset\n",
    "elevators = load_dataset('inria-soda/tabular-benchmark', data_files='reg_num/elevators.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elev = elevators['train'].to_pandas()\n",
    "elev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = elev.drop(columns=['Goal'])\n",
    "y = elev['Goal']\n",
    "\n",
    "lr_elev = LinearRegression()\n",
    "lr_elev.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_elev.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_elev.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(lr_elev.coef_, index=X.columns).sort_values().plot.barh(figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score is R^2 - the proportion of variance explained by the model\n",
    "lr_elev.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "mean_absolute_error(y, lr_elev.predict(X)), mean_squared_error(y, lr_elev.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_elev.predict(X.iloc[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions of Linear Regression\n",
    "\n",
    "- Linear relationship between the features and target variable\n",
    "- No multicollinearity - no correlation between the features\n",
    "- Homoscedasticity - the variance of the residuals is the same for all values of the target variable\n",
    "- No outliers - the residuals are normally distributed\n",
    "\n",
    "Also, generally you will want to scale the features before running linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_std = LinearRegression()\n",
    "lr_std.fit(X_scaled, y)\n",
    "lr_std.score(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(lr_std.coef_, index=X.columns).sort_values().plot.barh(figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(X, y)\n",
    "xgb.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Linear Regression\n",
    "\n",
    "Make a model to predict how much Titanic passengers paid for their tickets with Linear Regression. (Only use the numeric columns for the model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Logistic Regression Algorithm\n",
    "\n",
    "Even though logistic regression has \"regression\" in its name, it is used as a classification algorithm. It calculates the probability that a sample belongs to a class. Rather than fitting a line to the data, it fits an \"S\" shaped curve called the sigmoid function.\n",
    "\n",
    "The formula for logistic regression is as follows:\n",
    "\n",
    "$$y = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "where $y$ is the probability that a sample belongs to a class and $z$ is the linear combination of the features.\n",
    "\n",
    "The algorithm is as follows:\n",
    "\n",
    "- Initialize the weights.\n",
    "- Calculate the predicted values.\n",
    "- Calculate the error.\n",
    "- Update the weights.\n",
    "- Repeat the steps above until convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sigmoid function\n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = 1 / (1 + np.exp(-x))\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Example\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "log_data = pd.DataFrame({'x': [-2, -2.3, -2.1, -1, -.5,  0, .5, .7, 1, 2, 3],\n",
    "                    'y': [0, 0, 0, 0, 1, 0, 1,1, 1, 1, 1]})\n",
    "\n",
    "log_data.plot.scatter(x='x', y='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_r = LogisticRegression()\n",
    "log_r.fit(log_data[['x']], log_data['y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_r.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_r.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot fitted sigmoid function on top of data\n",
    "x = np.linspace(-3, 4, 100)\n",
    "y = 1 / (1 + np.exp(-(log_r.coef_[0][0] * x + log_r.intercept_[0])))\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(x, y)\n",
    "log_data.plot.scatter(x='x', y='y', ax=ax)\n",
    "# annotate above .5\n",
    "ax.annotate('Predict 1\\nright of this', xy=(-.31, .5), xytext=(2, .4), arrowprops={'arrowstyle': '->'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_r.predict([[-.3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real World Example with Eye movements\n",
    "\n",
    "From the website:\n",
    "\n",
    "The dataset consist of several assignments. Each assignment consists of a question followed by ten sentences (titles of news articles). One of the sentences is the correct answer to the question (C) and five of the sentences are irrelevant to the question (I). Four of the sentences are relevant to the question (R), but they do not answer it.\n",
    "\n",
    "- Features are in columns, feature vectors in rows.\n",
    "- Each assignment is a time sequence of 22-dimensional feature vectors.\n",
    "- The first column is the line number, second the assignment number and the next 22 columns (3 to 24) are the different features. Columns 25 to 27 contain extra information about the example. The training data set contains the classification label in the 28th column: \"0\" for irrelevant, \"1\" for relevant and \"2\" for the correct answer.\n",
    "- Each example (row) represents a single word. You are asked to return the classification of each read sentence.\n",
    "- The 22 features provided are commonly used in psychological studies on eye movement. All of them are not necessarily relevant in this context.\n",
    "\n",
    "The objective of the Challenge is to predict the classification labels (I, R, C).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.openml.org/da/1044\n",
    "from datasets import load_dataset\n",
    "eye = load_dataset('inria-soda/tabular-benchmark', data_files='clf_num/eye_movements.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_df = eye['train'].to_pandas()\n",
    "eye_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = eye_df.drop(columns=['label'])\n",
    "y = eye_df['label']\n",
    "std = StandardScaler()\n",
    "X_scaled = std.fit_transform(X)\n",
    "eye_log = LogisticRegression()\n",
    "eye_log.fit(X_scaled, y)\n",
    "eye_log.score(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(eye_log.coef_[0], index=X.columns).sort_values().plot.barh(figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Logistic Regression\n",
    "\n",
    "Create a logistic regression model to predict whether a Titanic passenger survives based on the numeric columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Algorithm\n",
    "\n",
    "Decision trees are a type of supervised learning algorithm that can be used for both classification and regression. They work by splitting the data into subsets based on the features. The goal is to split the data in a way that minimizes the entropy of the subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create \"decision stump\"\n",
    "## fit tree regressor to anscombe's quartet limit to 1 level\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor(max_depth=1)\n",
    "X = anscombe[['x']]\n",
    "y = anscombe['y1']\n",
    "dt.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the tree\n",
    "from sklearn.tree import plot_tree\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "_ = plot_tree(dt, ax=ax, feature_names=['x'], filled=True, fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the data and predictions on the same plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "anscombe.plot.scatter(x='x', y='y1', ax=ax, color='k')\n",
    "# plot the line\n",
    "x1 = np.linspace(4, 14, 100)\n",
    "y1 = dt.predict(x1.reshape(-1, 1))\n",
    "ax.plot(x1, y1, color='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now plot to two levels\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "anscombe.plot.scatter(x='x', y='y1', ax=ax, color='k')\n",
    "# plot the line\n",
    "dt2 = DecisionTreeRegressor(max_depth=2)\n",
    "dt2.fit(X, y)\n",
    "\n",
    "x1 = np.linspace(4, 14, 100)\n",
    "y1 = dt2.predict(x1.reshape(-1, 1))\n",
    "ax.plot(x1, y1, color='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now plot unlimited levels\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "anscombe.plot.scatter(x='x', y='y1', ax=ax, color='k')\n",
    "# plot the line\n",
    "dt3 = DecisionTreeRegressor(max_depth=None)\n",
    "dt3.fit(X, y)\n",
    "\n",
    "x1 = np.linspace(4, 14, 100)\n",
    "y1 = dt3.predict(x1.reshape(-1, 1))\n",
    "ax.plot(x1, y1, color='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real World with Aircraft Elevators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_elev = elev.drop(columns=['Goal'])\n",
    "y_elev = elev['Goal']\n",
    "dt_elev = DecisionTreeRegressor(max_depth=3)\n",
    "dt_elev.fit(X_elev, y_elev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the tree\n",
    "from sklearn.tree import plot_tree\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "_ = plot_tree(dt_elev, ax=ax, feature_names=X_elev.columns, filled=True, fontsize=10, precision=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_elev.score(X_elev, y_elev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_elev.score(X_elev, y_elev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over depths and plot the results\n",
    "scores = []\n",
    "for i in range(1, 20):\n",
    "    dt = DecisionTreeRegressor(max_depth=i)\n",
    "    dt.fit(X_elev, y_elev)\n",
    "    scores.append(dt.score(X_elev, y_elev))\n",
    "\n",
    "pd.Series(scores).plot.line(figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data and plot results of train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_elev, y_elev, random_state=42)\n",
    "test_scores = []\n",
    "train_scores = []\n",
    "for i in range(1, 20):\n",
    "    dt = DecisionTreeRegressor(max_depth=i)\n",
    "    dt.fit(X_train, y_train)\n",
    "    test_scores.append(dt.score(X_test, y_test))\n",
    "    train_scores.append(dt.score(X_train, y_train))\n",
    "\n",
    "ax = pd.DataFrame({'train': train_scores, 'test': test_scores}).plot.line(figsize=(8, 6))\n",
    "\n",
    "# annotate overfitting at 10, .7\n",
    "ax.annotate('Overfitting after here', xy=(10, .7), xytext=(12, .5), arrowprops={'arrowstyle': '->'})\n",
    "\n",
    "# set title\n",
    "ax.set_title('Validation Curve for Decision Tree')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if our model improves with a deeper tree\n",
    "dt_elev = DecisionTreeRegressor(max_depth=11)\n",
    "dt_elev.fit(X_train, y_train)\n",
    "dt_elev.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=3)\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep over depths and plot results\n",
    "test_scores = []\n",
    "train_scores = []\n",
    "for i in range(1, 20):\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_depth=i)\n",
    "    rf.fit(X_train, y_train)\n",
    "    test_scores.append(rf.score(X_test, y_test))\n",
    "    train_scores.append(rf.score(X_train, y_train))\n",
    "\n",
    "ax = pd.DataFrame({'train': train_scores, 'test': test_scores}).plot.line(figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=13, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an xgb regressor\n",
    "from xgboost import XGBRegressor\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Decision Trees\n",
    "\n",
    "Create a decision tree to predict survival on the titanic. See if you can determine the optimal depth of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion - Next Steps\n",
    "\n",
    "- Practice, practice, practice! - I recommend using your own data to practice.\n",
    "- Check out my Feature Engineering course.\n",
    "- Check out my XGBoost course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
